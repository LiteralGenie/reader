# The root folder should contain a folder for each series
# Each series folder should contain a folder for each chapter
# Each chapter should contain an image (.png, .jpg) for each page
# 
# Chapters and pages should be named alphabetically to match the reading order
# This program will also generate .sqlite files to store things like the OCR text, cover image, series name, etc
# 
# For example:
# root_image_folder
#     some_series
#          chap_01
#              01.png
#              02.png
#          chap_02
#              01.png
#              02.png
#     some_other_series
#          ...
root_image_folder = "./data/series"

api_port = 9494

# Path to model weights (*.pt). Leave blank ("") to use default weights (not recommended)
det_weights = ""
reco_weights = ""

# Model architecture name - https://mindee.github.io/doctr/modules/models.html
det_arch = "db_resnet50"
reco_arch = "parseq"

# Input size for detector model
# Images are resized to squares of side-length det_input_size before being OCR'd
det_input_size = 1024

# Input images are sliced into overlapping windows according to margin size before being OCR'd
margin_size = 100

# Images are resized to this width (if wider) before being OCR'd
# This improves results for images with large fonts (>60pt)
max_ocr_width = 1200

# Set to true if a NVIDIA GPU that supports CUDA is available for OCR
use_gpu_for_ocr = true

# Optional features that use a large language model (LLM):
#   - machine translation (MTL)
#   - de-prioritizing irrelevant dictionary definitions
# 
# llm_model_id should be the id of the hugging face repo hosting the model weights
# llm_model_file should be the name of the weights file
# 
# For example, given the below repo link
#     https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
# the model id is lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
# and under the "Files and Versions" section, we can find a .gguf file named Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
llm_model_id = "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF"
llm_model_file = "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# To run the LLM features on a GPU, set num_gpu_layers to a positive number
# (the bigger the better, but decrease it if there are out-of-memory errors)
# 
# Certain dependencies will need to be reinstalled before this takes effect
# 
# For NVIDIA GPUs specifically, run this:
#    ./core/venv/bin/python -m CMAKE_ARGS="-DGGML_CUDA=on" pip install --upgrade --force-reinstall --no-cache-dir python-doctr llama-cpp-python
# (python-doctr is reinstalled here to prevent version conflicts involving numpy 2.x.x)
# 
# For other GPUs or installation errors, see:
#   https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#installation
llm_num_gpu_layers = 0

# The reader attempts to breaks words into semantically meaningful parts and
# display a list of relevant definitions for each part, based mainly on part-of-speech.
# Some parts like 지 in 지어졌다 may match dozens of definitions, but only the top 5 are displayed.
# 
# If use_llm_for_definition_sort is enabled,
# then an LLM will pick the best N definitions,
# with the remaining slots filled according to the above logic
# 
# Enabling use_llm_for_definition_sort is not recommended unless
# gpu support is also enabled (see llm_num_gpu_layers)
use_llm_for_definition_sort = false
llm_num_definitions = 2

# If enabled, a machine translation is displayed for the overall sentence as well as each word
use_llm_for_mtl = true

# A cover image for each series will be automatically generated from one of the first pages
#   Images will be resized to fit max_auto_cover_x in width
#   Images will also be cropped to fit max_auto_cover_y height
# Set to 0 to disable
max_auto_cover_x = 0
max_auto_cover_y = 1000

max_chapter_size_bytes = 999_999_999
max_cover_image_size_bytes = 99_999_999

# Limits to apply when scanning for images from an url
max_import_requests_per_second = 999_999
max_import_bytes_per_second = 999_999_999

# This cap only counts images that would otherwise be included
# Images that are filtered out by the min dimension constraints are not counted
max_import_images_per_chapter = 99_999

max_mangadex_requests_per_second = 4
max_bakaupdate_requests_per_second = 1
