# The specified folder should contain a folder for each series.
# Each series folder should in turn contain a folder for each chapter.
# And each chapter folder should contain images (.png, .jpg) for each page
# 
# Chapters and pages should be named alphabetically to match the reading order
# 
# In each chapter folder, an additional database file (_reader_data.sqlite) may also be automatically generated.
# 
# For example:
# my_series_folder
#     some_series
#          chap_01
#              _reader_data.sqlite
#              01.png
#              02.png
#          chap_02
#              _reader_data.sqlite
#              01.png
#              02.png
#     some_other_series
#          ...
series_folder = "./data/series"

api_port = 9494

# Path to model weights (*.pt). Leave blank ("") to use default weights (not recommended)
det_weights = ""
reco_weights = ""

# Model architecture name - https://mindee.github.io/doctr/modules/models.html
det_arch = "db_resnet50"
reco_arch = "parseq"

# Input size for detector model
det_input_size = 1024

# Input images are sliced into overlapping windows according to margin size before being fed to the ocr models
margin_size = 100

# Images are resized to this width (if above) before being OCR'd
# This improves results for images with large fonts (>60pt)
max_ocr_width = 1200

# Set to true if a NVIDIA GPU (that supports CUDA) is available for inference
use_gpu = false

# Machine translations will not be enabled unless use_llm is true
#  
# llm_model_id should be the id of the hugging face repo hosting the model weights
# llm_model_file should be the name of the weights file
# 
# For example, given the below repo link
#     https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
# the model id is lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
# and under the "Files and Versions" section, we can find a .gguf file named Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
use_llm = true
llm_model_id = "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF"
llm_model_file = "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
